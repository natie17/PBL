{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natie17/PBL/blob/main/Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **í•„ìˆ˜ í•­ëª©**\n",
        "-ì¬ì‹œì‘ í•˜ë¼ëŠ” ë©”ì‹œì§€ ëœ¨ë©´ ê·¸ëƒ¥ ì¬ì‹œì‘ í›„ ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ ë„˜ì–´ê°€ì£¼ì„¸ìš”"
      ],
      "metadata": {
        "id": "PD_h4irf2fEL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JkVtbQlrxWDV",
        "outputId": "d3a33e20-4e37-48e5-c900-b139917d4489",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mediapipe==0.10.21\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting absl-py (from mediapipe==0.10.21)\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting attrs>=19.1.0 (from mediapipe==0.10.21)\n",
            "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting flatbuffers>=2.0 (from mediapipe==0.10.21)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting jax (from mediapipe==0.10.21)\n",
            "  Downloading jax-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe==0.10.21)\n",
            "  Downloading jaxlib-0.8.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting matplotlib (from mediapipe==0.10.21)\n",
            "  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting opencv-contrib-python (from mediapipe==0.10.21)\n",
            "  Downloading opencv_contrib_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe==0.10.21)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.21)\n",
            "  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting sentencepiece (from mediapipe==0.10.21)\n",
            "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe==0.10.21)\n",
            "  Downloading cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax->mediapipe==0.10.21)\n",
            "  Downloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax (from mediapipe==0.10.21)\n",
            "  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe==0.10.21)\n",
            "  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax (from mediapipe==0.10.21)\n",
            "  Downloading jax-0.7.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe==0.10.21)\n",
            "  Downloading jaxlib-0.7.2-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax (from mediapipe==0.10.21)\n",
            "  Downloading jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe==0.10.21)\n",
            "  Downloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting opt_einsum (from jax->mediapipe==0.10.21)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting scipy>=1.12 (from jax->mediapipe==0.10.21)\n",
            "  Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib->mediapipe==0.10.21)\n",
            "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->mediapipe==0.10.21)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->mediapipe==0.10.21)\n",
            "  Downloading fonttools-4.61.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (113 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib->mediapipe==0.10.21)\n",
            "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib->mediapipe==0.10.21)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pillow>=8 (from matplotlib->mediapipe==0.10.21)\n",
            "  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "Collecting pyparsing>=3 (from matplotlib->mediapipe==0.10.21)\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->mediapipe==0.10.21)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python (from mediapipe==0.10.21)\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pycparser (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.21)\n",
            "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.21)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
            "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.7.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (81.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m151.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (219 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m219.6/219.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.6/362.6 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.61.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m134.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m137.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: flatbuffers, six, sentencepiece, pyparsing, pycparser, protobuf, pillow, packaging, opt_einsum, numpy, kiwisolver, fonttools, cycler, attrs, absl-py, scipy, python-dateutil, opencv-contrib-python, ml_dtypes, contourpy, CFFI, sounddevice, matplotlib, jaxlib, jax, mediapipe\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 25.9.23\n",
            "    Uninstalling flatbuffers-25.9.23:\n",
            "      Successfully uninstalled flatbuffers-25.9.23\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.2.1\n",
            "    Uninstalling sentencepiece-0.2.1:\n",
            "      Successfully uninstalled sentencepiece-0.2.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.5\n",
            "    Uninstalling pyparsing-3.2.5:\n",
            "      Successfully uninstalled pyparsing-3.2.5\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.23\n",
            "    Uninstalling pycparser-2.23:\n",
            "      Successfully uninstalled pycparser-2.23\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: opt_einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.9\n",
            "    Uninstalling kiwisolver-1.4.9:\n",
            "      Successfully uninstalled kiwisolver-1.4.9\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.60.1\n",
            "    Uninstalling fonttools-4.60.1:\n",
            "      Successfully uninstalled fonttools-4.60.1\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "  Attempting uninstall: ml_dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.4\n",
            "    Uninstalling ml_dtypes-0.5.4:\n",
            "      Successfully uninstalled ml_dtypes-0.5.4\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.3\n",
            "    Uninstalling contourpy-1.3.3:\n",
            "      Successfully uninstalled contourpy-1.3.3\n",
            "  Attempting uninstall: CFFI\n",
            "    Found existing installation: cffi 2.0.0\n",
            "    Uninstalling cffi-2.0.0:\n",
            "      Successfully uninstalled cffi-2.0.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.7.2\n",
            "    Uninstalling jaxlib-0.7.2:\n",
            "      Successfully uninstalled jaxlib-0.7.2\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.7.2\n",
            "    Uninstalling jax-0.7.2:\n",
            "      Successfully uninstalled jax-0.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed CFFI-2.0.0 absl-py-2.3.1 attrs-25.4.0 contourpy-1.3.3 cycler-0.12.1 flatbuffers-25.9.23 fonttools-4.61.0 jax-0.7.1 jaxlib-0.7.1 kiwisolver-1.4.9 matplotlib-3.10.7 mediapipe-0.10.21 ml_dtypes-0.5.4 numpy-1.26.4 opencv-contrib-python-4.11.0.86 opt_einsum-3.4.0 packaging-25.0 pillow-12.0.0 protobuf-4.25.8 pycparser-2.23 pyparsing-3.2.5 python-dateutil-2.9.0.post0 scipy-1.16.3 sentencepiece-0.2.1 six-1.17.0 sounddevice-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "absl",
                  "attr",
                  "cv2",
                  "cycler",
                  "dateutil",
                  "flatbuffers",
                  "google",
                  "jax",
                  "jaxlib",
                  "kiwisolver",
                  "matplotlib",
                  "ml_dtypes",
                  "mpl_toolkits",
                  "numpy",
                  "opt_einsum",
                  "packaging",
                  "pyparsing",
                  "scipy",
                  "six"
                ]
              },
              "id": "ff70c721efb7438486eb7c433a5dedb9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"mediapipe ì„¤ì¹˜\"\"\"\n",
        "\n",
        "# MediaPipeì™€ í˜¸í™˜ë˜ëŠ” numpy ë²„ì „ì„ ê°•ì œ ì¬ì„¤ì¹˜\n",
        "!pip install numpy==1.26.4 mediapipe==0.10.21 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "lAxU7YtLyJXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8cb493-9ef6-41b9-d1e8-e19e3c21cfaa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "hVFoRnRYyLt5",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a8fd2f14-4f18-43a4-c556-c33a1120a0fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.233-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.7)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (12.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Collecting numpy>=1.23.0 (from ultralytics)\n",
            "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.233-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: numpy, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.6 ultralytics-8.3.233 ultralytics-thop-2.0.18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "2849399d5d444ca39c092e30bb8cc1da"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ì„¤ì¹˜ í™•ì¸\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "print(\"âœ… Mediapipeì™€ NumPy ë¡œë“œ ì„±ê³µ!\")\n",
        "print(f\"Mediapipe ë²„ì „: {mp.__version__}\")\n",
        "print(f\"NumPy ë²„ì „: {np.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qS_8esPhyNvO",
        "outputId": "718ef9e8-5708-4051-8a3e-9aa2c1909347"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Mediapipeì™€ NumPy ë¡œë“œ ì„±ê³µ!\n",
            "Mediapipe ë²„ì „: 0.10.21\n",
            "NumPy ë²„ì „: 2.2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "import mediapipe as mp\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Google Drive ë§ˆìš´íŠ¸\n",
        "drive.mount('/content/drive')\n",
        "print(\"âœ… Drive ë§ˆìš´íŠ¸ ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n",
        "\n",
        "# GPU/CPU ì„¤ì • (â­ ì´ ì½”ë“œê°€ device ë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤!)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"âœ… ì‚¬ìš© ì¥ì¹˜: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJNwmQz9x96N",
        "outputId": "54c21186-d263-4c93-adbc-49b43ca3d9a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Mounted at /content/drive\n",
            "âœ… Drive ë§ˆìš´íŠ¸ ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\n",
            "âœ… ì‚¬ìš© ì¥ì¹˜: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
        "# ==========================================\n",
        "\n",
        "LOAD_MODEL_PATH = '/content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras'\n",
        "X_TEST_PATH = '/content/drive/MyDrive/hand_project/Final_code/X_test_v3.npy'\n",
        "Y_TEST_PATH = '/content/drive/MyDrive/hand_project/Final_code/Y_test_v3.npy'\n",
        "\n",
        "loaded_model = None\n",
        "try:\n",
        "    loaded_model = load_model(LOAD_MODEL_PATH)\n",
        "    print(f\"âœ… Pinch ëª¨ë¸ V3 ë¡œë“œ ì„±ê³µ: {os.path.basename(LOAD_MODEL_PATH)}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "X_test = None\n",
        "Y_test = None\n",
        "try:\n",
        "    X_test = np.load(X_TEST_PATH)\n",
        "    Y_test = np.load(Y_TEST_PATH)\n",
        "    print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: (X: {X_test.shape}, Y: {Y_test.shape})\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨. 2ë‹¨ê³„ë¥¼ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”.\")\n",
        "\n",
        "print(\"\\nâœ… ë¶„ì„ ì´ˆê¸°í™” ì™„ë£Œ. 4ë‹¨ê³„ F1 ìŠ¤ì½”ì–´ ê³„ì‚°ì„ ì‹œì‘í•˜ì„¸ìš”.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLMhZ-ha2Vdg",
        "outputId": "dcb16082-da00-4642-cb2b-ea215e8d651b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Pinch ëª¨ë¸ V3 ë¡œë“œ ì„±ê³µ: skeleton_pinch_v3.keras\n",
            "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: (X: (60, 126), Y: (60,))\n",
            "\n",
            "âœ… ë¶„ì„ ì´ˆê¸°í™” ì™„ë£Œ. 4ë‹¨ê³„ F1 ìŠ¤ì½”ì–´ ê³„ì‚°ì„ ì‹œì‘í•˜ì„¸ìš”.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. ST-GCN ê´€ë ¨ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ ì •ì˜ (ì›ë³¸ AdaptiveSTGCN êµ¬ì¡°)\n",
        "# ğŸš¨ 3ï¸âƒ£ ë¸”ë¡ ì´ì „ì— ë°˜ë“œì‹œ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "# ==========================================\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch # PyTorchê°€ 1-2ì—ì„œ ì„í¬íŠ¸ë˜ì–´ ìˆì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ ë‹¤ì‹œ ëª…ì‹œí•©ë‹ˆë‹¤.\n",
        "\n",
        "# --- [2-1. ê·¸ë˜í”„(A) ì •ì˜] ---\n",
        "# MediaPipe ì† ê´€ì ˆ ì—°ê²° êµ¬ì¡° ì •ì˜ (21ê°œ ê´€ì ˆ)\n",
        "mediapipe_edges = [\n",
        "    (0,1), (1,2), (2,3), (3,4), (0,5), (5,6), (6,7), (7,8),\n",
        "    (0,9), (9,10), (10,11), (11,12), (0,13),(13,14),(14,15),(15,16),\n",
        "    (0,17),(17,18),(18,19),(19,20)\n",
        "]\n",
        "NUM_JOINTS = 21 # ê´€ì ˆ ìˆ˜\n",
        "\n",
        "# ì¸ì ‘ í–‰ë ¬(Adjacency Matrix) A ìƒì„±\n",
        "A = np.zeros((NUM_JOINTS, NUM_JOINTS))\n",
        "for i, j in mediapipe_edges:\n",
        "    A[i, j] = A[j, i] = 1 # ì—°ê²°ëœ ê´€ì ˆì€ 1\n",
        "for i in range(NUM_JOINTS):\n",
        "    A[i, i] = 1 # ìê¸° ìì‹ ê³¼ì˜ ì—°ê²° (self-loop)ì€ 1\n",
        "A = np.array([A], dtype=np.float32)\n",
        "\n",
        "\n",
        "# --- [2-2. Adaptive ST-GCN ëª¨ë¸ ì •ì˜] ---\n",
        "class AdaptiveSTGCNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, A, stride=1, residual=True):\n",
        "        super().__init__()\n",
        "        self.A = torch.tensor(A, dtype=torch.float32, requires_grad=False)\n",
        "        self.B = nn.Parameter(torch.zeros(A.shape, dtype=torch.float32)) # Adaptive Matrix B\n",
        "\n",
        "        self.gcn = nn.Conv2d(in_channels, out_channels, kernel_size=(1,1))\n",
        "        self.tcn = nn.Sequential(\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=(3,1), stride=(stride,1), padding=(1,0)),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = lambda x: x\n",
        "        else:\n",
        "            self.residual = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=(1,1), stride=(stride,1)),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        A_adaptive = self.A.to(x.device) + self.B.to(x.device)\n",
        "        # Spatial Graph Convolution\n",
        "        x_gcn = torch.einsum('nctv,vw->nctw', x, A_adaptive.squeeze(0))\n",
        "        x_gcn = self.gcn(x_gcn)\n",
        "        # Temporal Convolution\n",
        "        x_tcn = self.tcn(x_gcn)\n",
        "        return self.relu(x_tcn + self.residual(x))\n",
        "\n",
        "class AdaptiveSTGCN(nn.Module):\n",
        "    def __init__(self, A, num_classes=2, in_channels=7):\n",
        "        super().__init__()\n",
        "        # 7ch * 21 joints * 2 hands -> 294 features\n",
        "        self.data_bn = nn.BatchNorm1d(in_channels * 21 * 2)\n",
        "        self.layer1 = AdaptiveSTGCNBlock(in_channels, 64, A)\n",
        "        self.layer2 = AdaptiveSTGCNBlock(64, 128, A, stride=2)\n",
        "        self.layer3 = AdaptiveSTGCNBlock(128, 256, A, stride=2)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, T, V, M = x.shape\n",
        "        # Input: (N, C, T, V, M) -> (N, M*V*C, T)\n",
        "        x = x.permute(0, 4, 3, 1, 2).contiguous().view(N, M * V * C, T)\n",
        "        x = self.data_bn(x)\n",
        "        # Back to (N*M, C, T, V)\n",
        "        x = x.view(N, M, V, C, T).permute(0, 1, 3, 4, 2).contiguous().view(N * M, C, T, V)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        # Global Average Pooling (T, V ì°¨ì› ì¶•ì†Œ)\n",
        "        x = F.avg_pool2d(x, x.size()[2:])\n",
        "        # (N, M, Channel) -> ë‘ ì†ì˜ í‰ê· ì„ ì·¨í•¨\n",
        "        x = x.view(N, M, -1).mean(dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "print(\"âœ… ST-GCN ê´€ë ¨ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zozmJQsoajVs",
        "outputId": "0f7fb474-5f12-4988-b943-9be7f8884c90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ST-GCN ê´€ë ¨ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **í•™ìŠµ ìˆ˜í–‰**\n",
        "-ì‹¤ìŠµí•  ë•ŒëŠ” ê·¸ëƒ¥ ìœ„ì— ë¶ˆëŸ¬ì˜¤ê¸°ë§Œ í•´ì£¼ì„¸ìš”! -> ë°”ë¡œ ì½”ë“œ í•©ì²´ ë¸”ë¡ìœ¼ë¡œ"
      ],
      "metadata": {
        "id": "F5iIXmNT1k0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. ëª¨ë¸ V3 í•™ìŠµ ë° ì €ì¥ (ğŸš¨ ì˜¤íƒ€ ìˆ˜ì • ì™„ë£Œë¨)\n",
        "# ==========================================\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ğŸš¨ğŸš¨ ê²½ë¡œ ì„¤ì •\n",
        "BASE_PATH = '/content/drive/MyDrive/hand_project/Dataset'\n",
        "PINCH_PATH = os.path.join(BASE_PATH, 'pinch')\n",
        "NON_PINCH_PATH = os.path.join(BASE_PATH, 'train')\n",
        "MODEL_SAVE_DIR = '/content/drive/MyDrive/hand_project/Final_code'\n",
        "MODEL_PATH = os.path.join(MODEL_SAVE_DIR, 'skeleton_pinch_v3.keras')\n",
        "\n",
        "# ğŸš¨ğŸš¨ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ ê²½ë¡œ\n",
        "X_TEST_PATH = os.path.join(MODEL_SAVE_DIR, 'X_test_v3.npy')\n",
        "Y_TEST_PATH = os.path.join(MODEL_SAVE_DIR, 'Y_test_v3.npy') # ì˜¤íƒ€ ìˆ˜ì • ë°˜ì˜\n",
        "\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "print(f\"âœ… ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {MODEL_SAVE_DIR}\")\n",
        "\n",
        "# MediaPipe ì„¤ì •\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
        "\n",
        "# --- 2.1. íŒŒì¼ ë¡œë“œ ë° ì¶”ì¶œ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---\n",
        "def get_safe_image_files(folder_path):\n",
        "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
        "    safe_files = []\n",
        "    if os.path.exists(folder_path):\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for filename in files:\n",
        "                if filename.lower().endswith(valid_extensions):\n",
        "                    safe_files.append(os.path.join(root, filename))\n",
        "    return safe_files\n",
        "\n",
        "def extract_keypoints(file_path):\n",
        "    image = cv2.imread(file_path)\n",
        "    if image is None: return np.zeros(126)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(image_rgb)\n",
        "    keypoints = []\n",
        "    if results.multi_hand_landmarks:\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            for landmark in hand_landmarks.landmark:\n",
        "                keypoints.extend([landmark.x, landmark.y, landmark.z])\n",
        "    final_kp = np.zeros(126)\n",
        "    final_kp[:len(keypoints)] = keypoints[:126]\n",
        "    return final_kp\n",
        "\n",
        "# --- 2.2. ë°ì´í„° ë¡œë“œ ë° ì¦ê°• (ì¦ê°• ì œê±°) ---\n",
        "print(\"ğŸš€ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ ì‹œì‘...\")\n",
        "pinch_files = get_safe_image_files(PINCH_PATH)\n",
        "X_pinch = []\n",
        "print(\" Â  - Pinch ì¢Œí‘œ ì¶”ì¶œ (ì¦ê°• ì—†ìŒ) ì¤‘...\")\n",
        "for file_path in tqdm(pinch_files):\n",
        "    kp = extract_keypoints(file_path)\n",
        "    if np.any(kp):\n",
        "        X_pinch.append(kp)\n",
        "print(f\" Â  âœ… Pinch ë°ì´í„° í™•ë³´: {len(X_pinch)}ê°œ (ìµœì¢…)\")\n",
        "\n",
        "non_pinch_files = get_safe_image_files(NON_PINCH_PATH)\n",
        "NON_PINCH_TARGET_RATIO = 1.5\n",
        "target_non_pinch_count = int(len(X_pinch) * NON_PINCH_TARGET_RATIO)\n",
        "if len(non_pinch_files) > target_non_pinch_count:\n",
        "    random.seed(42)\n",
        "    selected_non_pinch = random.sample(non_pinch_files, target_non_pinch_count)\n",
        "else:\n",
        "    selected_non_pinch = non_pinch_files\n",
        "    target_non_pinch_count = len(selected_non_pinch)\n",
        "\n",
        "print(f\" Â  - Non-Pinch {len(non_pinch_files)}ê°œ ì¤‘ {target_non_pinch_count}ê°œ ìƒ˜í”Œë§ ì¤‘...\")\n",
        "X_non_pinch = []\n",
        "for file_path in tqdm(selected_non_pinch):\n",
        "    kp = extract_keypoints(file_path)\n",
        "    if np.any(kp):\n",
        "        X_non_pinch.append(kp)\n",
        "\n",
        "# --- 2.3. í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì„± ë° í•™ìŠµ ---\n",
        "X = np.array(X_pinch + X_non_pinch)\n",
        "y = np.array([1]*len(X_pinch) + [0]*len(X_non_pinch))\n",
        "\n",
        "# í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True, stratify=y)\n",
        "# ğŸš¨ğŸš¨ y_test ë³€ìˆ˜ëª…ì€ ì†Œë¬¸ì yë¡œ ì •ì˜ë©ë‹ˆë‹¤. ğŸš¨ğŸš¨\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"\\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: Train({len(X_train)}) / Val({len(X_val)}) / Test({len(X_test)})\")\n",
        "\n",
        "# ğŸš¨ğŸš¨ X_testì™€ y_testë¥¼ Driveì— ì €ì¥í•©ë‹ˆë‹¤! (ì˜¤íƒ€ ìˆ˜ì •ë¨) ğŸš¨ğŸš¨\n",
        "np.save(X_TEST_PATH, X_test)\n",
        "np.save(Y_TEST_PATH, y_test) # ğŸ‘ˆ ì—¬ê¸°ê°€ Y_testê°€ ì•„ë‹Œ y_testì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
        "print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {X_TEST_PATH}, {Y_TEST_PATH}\")\n",
        "\n",
        "\n",
        "# 2.4. ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ\n",
        "input_shape = X_train.shape[1]\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(126,)), Dense(512, activation='relu'), BatchNormalization(), Dropout(0.3),\n",
        "    Dense(256, activation='relu'), BatchNormalization(), Dropout(0.3),\n",
        "    Dense(128, activation='relu'), BatchNormalization(), Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=MODEL_PATH, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "print(f\"\\nğŸ”¥ í•™ìŠµ ì‹œì‘! (ì €ì¥ ê²½ë¡œ: {MODEL_PATH})\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks, verbose=1\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ‰ ëª¨ë¸ V3 í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aonoVWawxvkZ",
        "outputId": "0ac0ba46-12d9-4d49-b81c-16fa1f197eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ëª¨ë¸ ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/hand_project/Final_code\n",
            "ğŸš€ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ ì‹œì‘...\n",
            " Â  - Pinch ì¢Œí‘œ ì¶”ì¶œ (ì¦ê°• ì—†ìŒ) ì¤‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:35<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Â  âœ… Pinch ë°ì´í„° í™•ë³´: 174ê°œ (ìµœì¢…)\n",
            " Â  - Non-Pinch 2155ê°œ ì¤‘ 261ê°œ ìƒ˜í”Œë§ ì¤‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261/261 [00:42<00:00,  6.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: Train(278) / Val(60) / Test(60)\n",
            "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/hand_project/Final_code/X_test_v3.npy, /content/drive/MyDrive/hand_project/Final_code/Y_test_v3.npy\n",
            "\n",
            "ğŸ”¥ í•™ìŠµ ì‹œì‘! (ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras)\n",
            "Epoch 1/100\n",
            "\u001b[1m5/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5888 - loss: 0.8227\n",
            "Epoch 1: val_accuracy improved from -inf to 0.61667, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 188ms/step - accuracy: 0.6577 - loss: 0.6950 - val_accuracy: 0.6167 - val_loss: 0.6356\n",
            "Epoch 2/100\n",
            "\u001b[1m8/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8999 - loss: 0.2651\n",
            "Epoch 2: val_accuracy improved from 0.61667 to 0.65000, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.8969 - loss: 0.2760 - val_accuracy: 0.6500 - val_loss: 0.6163\n",
            "Epoch 3/100\n",
            "\u001b[1m7/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9060 - loss: 0.3418\n",
            "Epoch 3: val_accuracy improved from 0.65000 to 0.91667, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.9098 - loss: 0.3226 - val_accuracy: 0.9167 - val_loss: 0.6248\n",
            "Epoch 4/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9566 - loss: 0.2010\n",
            "Epoch 4: val_accuracy did not improve from 0.91667\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9566 - loss: 0.2003 - val_accuracy: 0.9167 - val_loss: 0.6187\n",
            "Epoch 5/100\n",
            "\u001b[1m8/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9697 - loss: 0.1490\n",
            "Epoch 5: val_accuracy improved from 0.91667 to 0.93333, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.9671 - loss: 0.1499 - val_accuracy: 0.9333 - val_loss: 0.5927\n",
            "Epoch 6/100\n",
            "\u001b[1m7/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9336 - loss: 0.1624\n",
            "Epoch 6: val_accuracy improved from 0.93333 to 0.95000, saving model to /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.9317 - loss: 0.1651 - val_accuracy: 0.9500 - val_loss: 0.5787\n",
            "Epoch 7/100\n",
            "\u001b[1m7/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9779 - loss: 0.1119\n",
            "Epoch 7: val_accuracy did not improve from 0.95000\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9801 - loss: 0.1071 - val_accuracy: 0.8167 - val_loss: 0.5777\n",
            "Epoch 8/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9764 - loss: 0.1069\n",
            "Epoch 8: val_accuracy did not improve from 0.95000\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9756 - loss: 0.1076 - val_accuracy: 0.7333 - val_loss: 0.5792\n",
            "Epoch 9/100\n",
            "\u001b[1m7/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9717 - loss: 0.0882 \n",
            "Epoch 9: val_accuracy did not improve from 0.95000\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9695 - loss: 0.0936 - val_accuracy: 0.5833 - val_loss: 0.5947\n",
            "Epoch 10/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9624 - loss: 0.0975\n",
            "Epoch 10: val_accuracy did not improve from 0.95000\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9618 - loss: 0.0988 - val_accuracy: 0.5000 - val_loss: 0.6244\n",
            "Epoch 11/100\n",
            "\u001b[1m5/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9759 - loss: 0.1252\n",
            "Epoch 11: val_accuracy did not improve from 0.95000\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9799 - loss: 0.1054 - val_accuracy: 0.4667 - val_loss: 0.6750\n",
            "Epoch 12/100\n",
            "\u001b[1m6/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9701 - loss: 0.0783\n",
            "Epoch 12: val_accuracy did not improve from 0.95000\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9700 - loss: 0.0856 - val_accuracy: 0.4500 - val_loss: 0.6792\n",
            "Epoch 13/100\n",
            "\u001b[1m5/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9749 - loss: 0.0755 \n",
            "Epoch 13: val_accuracy did not improve from 0.95000\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9755 - loss: 0.0730 - val_accuracy: 0.4500 - val_loss: 0.7003\n",
            "Epoch 14/100\n",
            "\u001b[1m5/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9752 - loss: 0.0654\n",
            "Epoch 14: val_accuracy did not improve from 0.95000\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9771 - loss: 0.0612 - val_accuracy: 0.4833 - val_loss: 0.6901\n",
            "Epoch 15/100\n",
            "\u001b[1m7/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9686 - loss: 0.1204 \n",
            "Epoch 15: val_accuracy did not improve from 0.95000\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9710 - loss: 0.1130 - val_accuracy: 0.4500 - val_loss: 0.7148\n",
            "Epoch 16/100\n",
            "\u001b[1m1/9\u001b[0m \u001b[32mâ”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 0.0292\n",
            "Epoch 16: val_accuracy did not improve from 0.95000\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9790 - loss: 0.0479 - val_accuracy: 0.4667 - val_loss: 0.7443\n",
            "Epoch 17/100\n",
            "\u001b[1m1/9\u001b[0m \u001b[32mâ”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9688 - loss: 0.0730\n",
            "Epoch 17: val_accuracy did not improve from 0.95000\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9806 - loss: 0.0571 - val_accuracy: 0.4667 - val_loss: 0.7487\n",
            "\n",
            "ğŸ‰ ëª¨ë¸ V3 í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 ìŠ¤ì½”ì–´ ê³„ì‚°"
      ],
      "metadata": {
        "id": "jx4wBbPp5NBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. ëª¨ë¸ ë¡œë“œ ë° ìµœì¢… ì„±ëŠ¥ í‰ê°€ (í˜¼ë™ í–‰ë ¬ í¬í•¨)\n",
        "# ==========================================\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "# ğŸš¨ğŸš¨ í˜¼ë™ í–‰ë ¬ ê³„ì‚°ì„ ìœ„í•´ confusion_matrix ì„í¬íŠ¸\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# ğŸš¨ğŸš¨ íŒŒì¼ ê²½ë¡œ ì„¤ì • (2ë‹¨ê³„ì—ì„œ ì €ì¥í•œ ê²½ë¡œì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤)\n",
        "LOAD_MODEL_PATH = '/content/drive/MyDrive/hand_project/Final_code/skeleton_pinch_v3.keras'\n",
        "X_TEST_PATH = '/content/drive/MyDrive/hand_project/Final_code/X_test_v3.npy'\n",
        "Y_TEST_PATH = '/content/drive/MyDrive/hand_project/Final_code/Y_test_v3.npy'\n",
        "\n",
        "# --- 3.1. ëª¨ë¸ ë¡œë“œ ---\n",
        "loaded_model = None\n",
        "try:\n",
        "    loaded_model = load_model(LOAD_MODEL_PATH)\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3.2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ---\n",
        "X_test = None\n",
        "Y_test = None\n",
        "try:\n",
        "    X_test = np.load(X_TEST_PATH)\n",
        "    Y_test = np.load(Y_TEST_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨. 2ë‹¨ê³„ë¥¼ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”.\")\n",
        "    exit()\n",
        "\n",
        "# ==========================================\n",
        "# 4. ëª¨ë¸ V3 ì„±ëŠ¥ í‰ê°€ (Loss, Accuracy ë° F1 ìŠ¤ì½”ì–´)\n",
        "# ==========================================\n",
        "print(\"\\nğŸš€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ëª¨ë¸ V3 ì„±ëŠ¥ í‰ê°€ ì‹œì‘...\")\n",
        "\n",
        "# 1. Keras evaluateë¥¼ í†µí•œ Lossì™€ Accuracy ê³„ì‚°\n",
        "loss, accuracy = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "# 2. ëª¨ë¸ ì˜ˆì¸¡ (F1 ìŠ¤ì½”ì–´ ê³„ì‚°ì„ ìœ„í•¨)\n",
        "Y_prob = loaded_model.predict(X_test)\n",
        "Y_pred = (Y_prob > 0.5).astype(int)\n",
        "\n",
        "# 3. F1, Precision, Recall ê³„ì‚°\n",
        "try:\n",
        "    f1 = f1_score(Y_test, Y_pred, pos_label=1)\n",
        "    precision = precision_score(Y_test, Y_pred, pos_label=1)\n",
        "    recall = recall_score(Y_test, Y_pred, pos_label=1)\n",
        "\n",
        "    # ğŸš¨ğŸš¨ NEW: Confusion Matrix ê³„ì‚°\n",
        "    cm = confusion_matrix(Y_test, Y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel() # TN, FP, FN, TP ê°’ì„ ì¶”ì¶œ\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"âŒ ì„±ëŠ¥ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 4. ê²°ê³¼ ì¶œë ¥\n",
        "print(\"\\n==============================================\")\n",
        "print(f\"ğŸ“Š ëª¨ë¸ V3 ìµœì¢… ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ (ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(X_test)})\")\n",
        "print(\"==============================================\")\n",
        "print(f\"ì†ì‹¤ (Loss):          {loss:.4f}\")\n",
        "print(f\"ì •í™•ë„ (Accuracy):    {accuracy:.4f}\")\n",
        "print(\"----------------------------------------------\")\n",
        "print(f\"ì •ë°€ë„ (Precision): {precision:.4f}\")\n",
        "print(f\"ì¬í˜„ìœ¨ (Recall):   {recall:.4f}\")\n",
        "print(f\"F1 ìŠ¤ì½”ì–´:          {f1:.4f}\")\n",
        "print(\"----------------------------------------------\")\n",
        "print(\"\\n--- í˜¼ë™ í–‰ë ¬ (Confusion Matrix) ---\")\n",
        "print(\"   [ì‹¤ì œ NonPinch] [ì‹¤ì œ Pinch]\")\n",
        "print(f\"NonPinch ì˜ˆì¸¡: [{tn}            {fp}          ] (TN, FP)\")\n",
        "print(f\"Pinch ì˜ˆì¸¡:    [{fn}            {tp}          ] (FN, TP)\")\n",
        "print(\"\\n----------------------------------------------\")\n",
        "print(\"  TN (True Negative): ë¹„ê¼¬ì§‘ê¸°ë¥¼ ë¹„ê¼¬ì§‘ê¸°ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ìˆ˜\")\n",
        "print(\"  FP (False Positive): ë¹„ê¼¬ì§‘ê¸°ë¥¼ ê¼¬ì§‘ê¸°ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ìˆ˜ (ì˜¤íƒì§€)\")\n",
        "print(\"  FN (False Negative): ê¼¬ì§‘ê¸°ë¥¼ ë¹„ê¼¬ì§‘ê¸°ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ìˆ˜ (ë†“ì¹œ ìˆ˜)\")\n",
        "print(\"  TP (True Positive): ê¼¬ì§‘ê¸°ë¥¼ ê¼¬ì§‘ê¸°ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ìˆ˜\")\n",
        "print(\"==============================================\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DKs712kNLeXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97025dc8-337d-4877-eafe-0096c6d0993d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸš€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ëª¨ë¸ V3 ì„±ëŠ¥ í‰ê°€ ì‹œì‘...\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367ms/step\n",
            "\n",
            "==============================================\n",
            "ğŸ“Š ëª¨ë¸ V3 ìµœì¢… ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ (ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: 60)\n",
            "==============================================\n",
            "ì†ì‹¤ (Loss):          0.5879\n",
            "ì •í™•ë„ (Accuracy):    0.8833\n",
            "----------------------------------------------\n",
            "ì •ë°€ë„ (Precision): 0.8065\n",
            "ì¬í˜„ìœ¨ (Recall):   0.9615\n",
            "F1 ìŠ¤ì½”ì–´:          0.8772\n",
            "----------------------------------------------\n",
            "\n",
            "--- í˜¼ë™ í–‰ë ¬ (Confusion Matrix) ---\n",
            "   [ì‹¤ì œ NonPinch] [ì‹¤ì œ Pinch]\n",
            "NonPinch ì˜ˆì¸¡: [28            6          ] (TN, FP)\n",
            "Pinch ì˜ˆì¸¡:    [1            25          ] (FN, TP)\n",
            "\n",
            "----------------------------------------------\n",
            "  TN (True Negative): ë¹„ê¼¬ì§‘ê¸°ë¥¼ ë¹„ê¼¬ì§‘ê¸°ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ìˆ˜\n",
            "  FP (False Positive): ë¹„ê¼¬ì§‘ê¸°ë¥¼ ê¼¬ì§‘ê¸°ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ìˆ˜ (ì˜¤íƒì§€)\n",
            "  FN (False Negative): ê¼¬ì§‘ê¸°ë¥¼ ë¹„ê¼¬ì§‘ê¸°ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ìˆ˜ (ë†“ì¹œ ìˆ˜)\n",
            "  TP (True Positive): ê¼¬ì§‘ê¸°ë¥¼ ê¼¬ì§‘ê¸°ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ìˆ˜\n",
            "==============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì½”ë“œ í•©ì²´**"
      ],
      "metadata": {
        "id": "CtuGa0rPOiBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3-1. ëª¨ë¸ ë¡œë“œ ë° ìƒìˆ˜ ì •ì˜ (Fusion ì‹¤í–‰ ì¤€ë¹„)\n",
        "# ==========================================\n",
        "\n",
        "# ğŸš¨ğŸš¨ğŸš¨ ê²½ë¡œ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ ğŸš¨ğŸš¨ğŸš¨\n",
        "MODEL_SAVE_DIR = '/content/drive/MyDrive/hand_project/Final_code'\n",
        "STGCN_MODEL_DIR = '/content/drive/MyDrive/Pinching_data/stgcn/'\n",
        "LOAD_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, 'skeleton_pinch_v3.keras')\n",
        "STGCN_MODEL_PATH = os.path.join(STGCN_MODEL_DIR, 'best_model_adaptive.pth')\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ì°¸ê³ : INPUT_VIDEO_PATH ë³€ìˆ˜ëŠ” 3-2 ì…€ì—ì„œ ì •ì˜ë©ë‹ˆë‹¤.\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Fusion ê°€ì¤‘ì¹˜ ë° ìƒìˆ˜\n",
        "WEIGHT_DNN = 0.6\n",
        "WEIGHT_STGCN = 0.4\n",
        "SEQ_LEN = 30\n",
        "NUM_CHANNELS_STGCN = 7\n",
        "\n",
        "# ë¶„ì„ ìƒìˆ˜ (DNN/Logicìš©)\n",
        "DISTANCE_THRESHOLD = 0.05\n",
        "FRAME_SKIP_INTERVAL = 3\n",
        "MIN_CLOSING_RATE = 0.005\n",
        "\n",
        "# â­ ëˆ„ë½ëœ í•µì‹¬ ìƒìˆ˜ ì¶”ê°€ â­\n",
        "CLS_ID_FOR_HAND = 0 # YOLOv8ì—ì„œ ì†(hand)ì„ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤ ID (ì¼ë°˜ YOLOv8nì—ì„œëŠ” ë³´í†µ 0ì´ 'person'ì´ì§€ë§Œ, hand-detection ëª¨ë¸ ì‚¬ìš©ì„ ê°€ì •)\n",
        "SAVE_DIR = '/content/drive/MyDrive/hand_project/video_check' # ì¶œë ¥ ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n",
        "\n",
        "# --- ëª¨ë¸ ë¡œë“œ ---\n",
        "dnn_model = None\n",
        "stgcn_model = None\n",
        "\n",
        "try:\n",
        "    # 1. Keras DNN ëª¨ë¸ ë¡œë“œ\n",
        "    dnn_model = load_model(LOAD_MODEL_PATH)\n",
        "    print(f\"âœ… Pinch ëª¨ë¸ V3 (DNN) ë¡œë“œ ì„±ê³µ: {os.path.basename(LOAD_MODEL_PATH)}\")\n",
        "\n",
        "    # 2. PyTorch STGCN ëª¨ë¸ ë¡œë“œ\n",
        "    stgcn_model = AdaptiveSTGCN(A=A, num_classes=2, in_channels=NUM_CHANNELS_STGCN).to(device)\n",
        "    state_dict = torch.load(STGCN_MODEL_PATH, map_location=device)\n",
        "\n",
        "    if all(k.startswith('module.') for k in state_dict.keys()):\n",
        "        state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
        "\n",
        "    stgcn_model.load_state_dict(state_dict)\n",
        "    stgcn_model.eval()\n",
        "    print(f\"âœ… ST-GCN ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {os.path.basename(STGCN_MODEL_PATH)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ST-GCN ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. Fusionì€ DNN ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")\n",
        "    stgcn_model = None\n",
        "\n",
        "# YOLO ëª¨ë¸ ë¡œë“œ\n",
        "try:\n",
        "    yolo_model = YOLO('yolov8n.pt')\n",
        "    print(\"âœ… YOLOv8 ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ YOLOv8 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    exit()\n",
        "\n",
        "# MediaPipe ì´ˆê¸°í™” ë° ê²½ë¡œ ì„¤ì •\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "hands = mp_hands.Hands(\n",
        "    static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
        ")\n",
        "CAPTURED_FRAMES_DIR = '/content/drive/MyDrive/hand_project/Captured_Pinch_Frames_1'\n",
        "os.makedirs(CAPTURED_FRAMES_DIR, exist_ok=True)\n",
        "os.makedirs(SAVE_DIR, exist_ok=True) # SAVE_DIR í´ë” ìƒì„± ë¡œì§ ì¶”ê°€\n",
        "print(f\"âœ… ìº¡ì²˜ í”„ë ˆì„ ì €ì¥ í´ë”: {CAPTURED_FRAMES_DIR}\")"
      ],
      "metadata": {
        "id": "tj6kZVseSrp7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4e17ae-7262-4717-f577-e0c03fc8d8e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Pinch ëª¨ë¸ V3 (DNN) ë¡œë“œ ì„±ê³µ: skeleton_pinch_v3.keras\n",
            "âœ… ST-GCN ëª¨ë¸ ë¡œë“œ ì„±ê³µ: best_model_adaptive.pth\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 88.3MB/s 0.1s\n",
            "âœ… YOLOv8 ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\n",
            "âœ… ìº¡ì²˜ í”„ë ˆì„ ì €ì¥ í´ë”: /content/drive/MyDrive/hand_project/Captured_Pinch_Frames_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì½”ë“œ í•©ì²´ í›„ í…ŒìŠ¤íŠ¸**\n",
        "\n",
        "*   ë“œë¼ì´ë¸Œì— ìˆëŠ” ì˜ìƒ -> input_data_pathì— ì§ì ‘ ê²½ë¡œ, íŒŒì¼ ëª… ì…ë ¥\n",
        "*   ë…¸íŠ¸ë¶ì—ì„œ ì—…ë¡œë“œ -> ë²„íŠ¼ ëˆ„ë¥´ê³  ì—…ë¡œë“œ"
      ],
      "metadata": {
        "id": "_0U-bok_PHN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3-A. ğŸ“¹ ì…ë ¥ ì˜ìƒ ê²½ë¡œ ì„¤ì • (íŒŒì¼ ì—…ë¡œë“œ)\n",
        "# ==========================================\n",
        "from google.colab import files\n",
        "\n",
        "# ğŸš¨ INPUT_VIDEO_PATH ë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "INPUT_VIDEO_PATH = \"\"\n",
        "\n",
        "print(\"ğŸ“¹ ë¶„ì„í•  ë¹„ë””ì˜¤ íŒŒì¼(.mp4 ë˜ëŠ” .MOV)ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ Drive ê²½ë¡œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.\")\n",
        "print(\"  (Drive ê²½ë¡œë¥¼ ì‚¬ìš©í•  ê²½ìš°, ì•„ë˜ ì½”ë“œ ëŒ€ì‹  ì§ì ‘ ë³€ìˆ˜ì— ê²½ë¡œë¥¼ í• ë‹¹í•´ì•¼ í•©ë‹ˆë‹¤.)\")\n",
        "\n",
        "# íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯ì„ ì‚¬ìš©í•˜ì—¬ ê²½ë¡œë¥¼ ì–»ëŠ” ë°©ì‹\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        # ì—…ë¡œë“œëœ íŒŒì¼ ì´ë¦„ì„ ê²½ë¡œë¡œ ì‚¬ìš©\n",
        "        INPUT_VIDEO_PATH = list(uploaded.keys())[0]\n",
        "        print(f\"\\nâœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ. INPUT_VIDEO_PATH = '{INPUT_VIDEO_PATH}'\")\n",
        "    else:\n",
        "        # ì—…ë¡œë“œí•˜ì§€ ì•Šì€ ê²½ìš°, ìˆ˜ë™ìœ¼ë¡œ Drive ê²½ë¡œë¥¼ ì…ë ¥í•´ì•¼ í•¨\n",
        "        print(\"\\nâš ï¸ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. INPUT_VIDEO_PATH ë³€ìˆ˜ì— ìˆ˜ë™ìœ¼ë¡œ Drive ê²½ë¡œë¥¼ í• ë‹¹í•´ì£¼ì„¸ìš”.\")\n",
        "        # ì´ì „ì— ì‚¬ìš©í•˜ë˜ Drive ê²½ë¡œë¥¼ ì„ì‹œë¡œ ë‚¨ê²¨ë‘ê±°ë‚˜ ì‚¬ìš©ìì—ê²Œ ì…ë ¥ì„ ë°›ë„ë¡ ì•ˆë‚´\n",
        "        INPUT_VIDEO_PATH = '/content/drive/MyDrive/hand_project/Dataset/youtube_test3.mp4'\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\n",
        "# ìµœì¢…ì ìœ¼ë¡œ INPUT_VIDEO_PATHê°€ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
        "if not INPUT_VIDEO_PATH:\n",
        "    # ğŸš¨ğŸš¨ ì‚¬ìš©ìì—ê²Œ ìµœì¢…ì ìœ¼ë¡œ ê²½ë¡œë¥¼ ì…ë ¥í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ì½”ë“œ (ì§ì ‘ ì…ë ¥ í›„ ì‹¤í–‰)\n",
        "    INPUT_VIDEO_PATH = input(\"/content/drive/MyDrive/video.mp4) ì—”í„°ë¥¼ ëˆ„ë¥¸ ë’¤, ì´ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”: \")\n",
        "\n",
        "print(f\"\\nğŸš€ ìµœì¢… ì…ë ¥ ì˜ìƒ ê²½ë¡œ: {INPUT_VIDEO_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "id": "dgLv8gtcbWcc",
        "outputId": "d51785ff-cdba-45d1-89f0-bf7560ded521"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¹ ë¶„ì„í•  ë¹„ë””ì˜¤ íŒŒì¼(.mp4 ë˜ëŠ” .MOV)ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ Drive ê²½ë¡œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.\n",
            "  (Drive ê²½ë¡œë¥¼ ì‚¬ìš©í•  ê²½ìš°, ì•„ë˜ ì½”ë“œ ëŒ€ì‹  ì§ì ‘ ë³€ìˆ˜ì— ê²½ë¡œë¥¼ í• ë‹¹í•´ì•¼ í•©ë‹ˆë‹¤.)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2296f091-d603-4982-a7cf-d4b6b3591ebd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2296f091-d603-4982-a7cf-d4b6b3591ebd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âš ï¸ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. INPUT_VIDEO_PATH ë³€ìˆ˜ì— ìˆ˜ë™ìœ¼ë¡œ Drive ê²½ë¡œë¥¼ í• ë‹¹í•´ì£¼ì„¸ìš”.\n",
            "\n",
            "ğŸš€ ìµœì¢… ì…ë ¥ ì˜ìƒ ê²½ë¡œ: /content/drive/MyDrive/hand_project/Dataset/youtube_test3.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. ì˜ìƒ ë¶„ì„ ì‹¤í–‰ ë° Fusion ì˜ˆì¸¡ (ì €ì¥/í™”ì§ˆ ê°œì„ )\n",
        "# ==========================================\n",
        "\n",
        "cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(f\"âŒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_VIDEO_PATH}\")\n",
        "    cap.release()\n",
        "    exit()\n",
        "\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "original_filename = os.path.basename(INPUT_VIDEO_PATH).split('.')[0]\n",
        "OUTPUT_VIDEO_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_V3.mp4\")\n",
        "PREDICTIONS_SAVE_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_v3_strict_predictions.npy\")\n",
        "\n",
        "# ğŸš¨ğŸš¨ğŸš¨ ì½”ë±/í™”ì§ˆ ê°œì„ : XVID ìœ ì§€, ë˜ëŠ” MP4Vë¡œ ë³€ê²½ ì‹œë„ (Colab í˜¸í™˜ì„± ë¬¸ì œ ìˆìŒ) ğŸš¨ğŸš¨ğŸš¨\n",
        "# XVID: ê°€ì¥ í˜¸í™˜ì„±ì´ ì¢‹ìœ¼ë‚˜ í™”ì§ˆì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ. (ê¾¸ë¬¼ê±°ë¦¬ëŠ” í˜„ìƒì€ ì˜ˆì¸¡ ì†ë„ ë¬¸ì œ)\n",
        "# MP4V: XVIDë³´ë‹¤ í™”ì§ˆì´ ì¢‹ìœ¼ë‚˜ Colab í™˜ê²½ì—ì„œ ì¬ìƒì´ ì•ˆ ë  ìˆ˜ ìˆìŒ.\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n",
        "\n",
        "if not out.isOpened():\n",
        "    print(f\"âŒ VideoWriter ì´ˆê¸°í™” ì‹¤íŒ¨! ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì½”ë±ì„ ë³€ê²½í•˜ì„¸ìš”: {OUTPUT_VIDEO_PATH}\")\n",
        "    # ì½”ë±ì„ MP4Vë¡œ ì¬ì‹œë„\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "    out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n",
        "    if not out.isOpened():\n",
        "        print(\"âŒ MP4V ì½”ë±ìœ¼ë¡œë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.\")\n",
        "        cap.release()\n",
        "        exit()\n",
        "\n",
        "print(f\"ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘ ({total_frames} í”„ë ˆì„, Skip Rate: 1 / {FRAME_SKIP_INTERVAL})\")\n",
        "print(f\"ğŸ’¾ ì¶œë ¥ ê²½ë¡œ í™•ì¸: {OUTPUT_VIDEO_PATH}\")\n",
        "pbar = tqdm(total=total_frames // FRAME_SKIP_INTERVAL)\n",
        "\n",
        "frame_counter = 0\n",
        "prev_dist = None\n",
        "stgcn_sequence = []\n",
        "all_predictions = []\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # ì˜ìƒ ë°©í–¥ ì •ìƒí™” (ìˆ˜ì§ ë°˜ì „)\n",
        "    frame = cv2.flip(frame, 0)\n",
        "\n",
        "    frame_idx_in_video = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "    frame_counter += 1\n",
        "    current_frame_prediction = 0\n",
        "\n",
        "    if frame_counter % FRAME_SKIP_INTERVAL != 0:\n",
        "        out.write(frame)\n",
        "        all_predictions.append(0)\n",
        "        continue\n",
        "\n",
        "    # --- ë©”ì¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì§„ì… ---\n",
        "    yolo_results = yolo_model(frame, verbose=False)\n",
        "\n",
        "    is_object_present_in_frame = False\n",
        "    is_hand_detected_yolo = False\n",
        "    current_keypoints_frame = np.zeros((2, NUM_JOINTS, 4), dtype=np.float32)\n",
        "    final_kp_dnn = np.zeros(126)\n",
        "\n",
        "    cls_id_for_hand = CLS_ID_FOR_HAND # CLS_ID_FOR_HAND ì‚¬ìš©\n",
        "    hand_boxes = []\n",
        "\n",
        "    for r in yolo_results:\n",
        "        for i, cls_id in enumerate(r.boxes.cls.cpu().numpy().astype(int)):\n",
        "            box = r.boxes.xyxy.cpu().numpy().astype(int)[i]\n",
        "\n",
        "            if cls_id == cls_id_for_hand:\n",
        "                hand_boxes.append(box)\n",
        "                is_hand_detected_yolo = True\n",
        "            elif cls_id != 0:\n",
        "                 is_object_present_in_frame = True\n",
        "\n",
        "    if not is_hand_detected_yolo:\n",
        "        cv2.putText(frame, \"No Hand Detected (YOLO)\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "        out.write(frame)\n",
        "        all_predictions.append(0)\n",
        "        pbar.update(1)\n",
        "        continue\n",
        "\n",
        "    is_hand_detected_mediapipe = False\n",
        "\n",
        "    for box in hand_boxes:\n",
        "        x1, y1, x2, y2 = box\n",
        "        cropped_img = frame[y1:y2, x1:x2]\n",
        "        if cropped_img.size == 0 or x2 <= x1 or y2 <= y1:\n",
        "            continue\n",
        "\n",
        "        mp_results = hands.process(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        if mp_results.multi_hand_landmarks:\n",
        "            is_hand_detected_mediapipe = True\n",
        "\n",
        "            for hand_idx, hand_landmarks in enumerate(mp_results.multi_hand_landmarks):\n",
        "                if hand_idx >= 2: continue\n",
        "\n",
        "                kp_list_dnn = []\n",
        "                for lm in hand_landmarks.landmark:\n",
        "                    kp_list_dnn.extend([lm.x, lm.y, lm.z])\n",
        "\n",
        "                if hand_idx == 0:\n",
        "                    final_kp_dnn[:len(kp_list_dnn)] = kp_list_dnn[:126]\n",
        "\n",
        "                lm_xyz = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
        "                thumb_tip = lm_xyz[4]\n",
        "                index_tip = lm_xyz[8]\n",
        "                dist = np.linalg.norm(thumb_tip - index_tip)\n",
        "\n",
        "                if dist < 0.25:\n",
        "                    modified_dist = dist * 0.2\n",
        "                else:\n",
        "                    modified_dist = dist\n",
        "\n",
        "                dist_feat = np.full((NUM_JOINTS, 1), modified_dist)\n",
        "                hand_kp_extended = np.concatenate((lm_xyz, dist_feat), axis=1)\n",
        "                current_keypoints_frame[hand_idx] = hand_kp_extended\n",
        "\n",
        "                transformed_hand_landmarks = landmark_pb2.NormalizedLandmarkList()\n",
        "                transformed_hand_landmarks.landmark.extend([\n",
        "                    landmark_pb2.NormalizedLandmark(\n",
        "                        x=(lm.x * (x2 - x1) + x1) / width,\n",
        "                        y=(lm.y * (y2 - y1) + y1) / height,\n",
        "                        z=lm.z\n",
        "                    ) for lm in hand_landmarks.landmark\n",
        "                ])\n",
        "\n",
        "                mp_drawing.draw_landmarks(frame, transformed_hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
        "                                          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "                                          mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "            if is_hand_detected_mediapipe:\n",
        "                prev_dist = dist\n",
        "\n",
        "            final_prob = 0.0\n",
        "            fusion_reason = \"DNN Only\"\n",
        "            ai_prob_dnn = dnn_model.predict(final_kp_dnn.reshape(1, -1), verbose=0)[0][0]\n",
        "\n",
        "            if stgcn_model is None:\n",
        "                final_prob = ai_prob_dnn\n",
        "                fusion_reason = \"DNN Only (STGCN Fail)\"\n",
        "            else:\n",
        "                stgcn_sequence.append(current_keypoints_frame)\n",
        "                if len(stgcn_sequence) > SEQ_LEN:\n",
        "                    stgcn_sequence.pop(0)\n",
        "\n",
        "                if len(stgcn_sequence) == SEQ_LEN:\n",
        "                    seq_arr = np.array(stgcn_sequence, dtype=np.float32)\n",
        "                    seq_arr = seq_arr.reshape(SEQ_LEN, 2, NUM_JOINTS, 4)\n",
        "                    input_data = np.transpose(seq_arr, (3, 0, 2, 1))\n",
        "                    input_data = np.expand_dims(input_data, axis=0)\n",
        "\n",
        "                    wrist_id = 0\n",
        "                    for m in range(2):\n",
        "                        wrist_xyz = input_data[0, :3, :, wrist_id, m].copy()\n",
        "                        input_data[0, :3, :, :, m] -= wrist_xyz[:, :, None]\n",
        "\n",
        "                    max_val = np.max(np.abs(input_data))\n",
        "                    if max_val > 0:\n",
        "                        input_data /= max_val\n",
        "\n",
        "                    velocity = np.zeros((1, 3, SEQ_LEN, NUM_JOINTS, 2), dtype=np.float32)\n",
        "                    velocity[:, :, 1:, :, :] = input_data[:, :3, 1:, :, :] - input_data[:, :3, :-1, :, :]\n",
        "                    final_input = np.concatenate((input_data, velocity), axis=1)\n",
        "\n",
        "                    input_tensor = torch.tensor(final_input, dtype=torch.float32).to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        outputs = stgcn_model(input_tensor)\n",
        "                        probabilities = F.softmax(outputs, dim=1)\n",
        "                        prob_stgcn = probabilities[0, 1].item()\n",
        "\n",
        "                    final_prob = (ai_prob_dnn * WEIGHT_DNN) + (prob_stgcn * WEIGHT_STGCN)\n",
        "                    fusion_reason = f\"Fusion ({WEIGHT_DNN:.1f}:{WEIGHT_STGCN:.1f})\"\n",
        "                else:\n",
        "                    final_prob = ai_prob_dnn\n",
        "                    fusion_reason = \"DNN Only (Buffering)\"\n",
        "\n",
        "            is_closing = False\n",
        "            if prev_dist is not None and (prev_dist - dist) > MIN_CLOSING_RATE:\n",
        "                is_closing = True\n",
        "\n",
        "            label = \"Non-Pinch\"\n",
        "            color = (255, 0, 0)\n",
        "            reason_text = \"Non-Pinch\"\n",
        "            current_frame_prediction = 0\n",
        "\n",
        "            if final_prob > 0.5:\n",
        "                label = f\"FINAL: PINCH ({final_prob*100:.1f}%)\"\n",
        "                color = (0, 255, 0)\n",
        "                reason_text = fusion_reason\n",
        "                current_frame_prediction = 1\n",
        "\n",
        "            elif dist < DISTANCE_THRESHOLD and is_closing and is_object_present_in_frame:\n",
        "                label = f\"FINAL: PINCH (Logic)\"\n",
        "                color = (0, 255, 255)\n",
        "                reason_text = f\"Closing + Object\"\n",
        "                current_frame_prediction = 1\n",
        "\n",
        "            display_text = f\"{label} ({reason_text})\"\n",
        "            cv2.rectangle(frame, (x1, y2 - 40), (x1 + 550, y2), color, -1)\n",
        "            cv2.putText(frame, display_text, (x1 + 10, y2 - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "    pbar.update(1)\n",
        "    all_predictions.append(current_frame_prediction)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "pbar.close()\n",
        "\n",
        "np.save(PREDICTIONS_SAVE_PATH, np.array(all_predictions))\n",
        "\n",
        "print(f\"\\nâœ¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {OUTPUT_VIDEO_PATH}\")\n",
        "print(f\"âœ… Fusion ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {PREDICTIONS_SAVE_PATH}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OpXUop12PG7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b94d55d-9d48-41a0-b68f-7fe1a2e0434e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘ (241 í”„ë ˆì„, Skip Rate: 1 / 3)\n",
            "ğŸ’¾ ì¶œë ¥ ê²½ë¡œ í™•ì¸: /content/drive/MyDrive/hand_project/video_check/youtube_test3_V3.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:08<00:00,  9.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: /content/drive/MyDrive/hand_project/video_check/youtube_test3_V3.mp4\n",
            "âœ… Fusion ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/hand_project/video_check/youtube_test3_v3_strict_predictions.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **í”„ë ˆì„ ê¸°ì¤€ ë¶„ì„**\n",
        "\n",
        "ë°˜ì „ì€ ì˜ìƒ ê¸°ì¤€ì´ë¼ ëŒë ¤ë³´ê³  ë’¤ì§‘ì–´ì ¸ìˆìŒ ìƒëµí•´ë„ ë  ë“¯"
      ],
      "metadata": {
        "id": "np3LeG_569-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# í”„ë ˆì„ í´ë” ê²½ë¡œ ì„¤ì •\n",
        "# ==========================================\n",
        "# ğŸš¨ ì´ ë³€ìˆ˜ì— ë¶„í• ëœ í”„ë ˆì„ ì´ë¯¸ì§€ê°€ ë“¤ì–´ìˆëŠ” í´ë”ì˜ ê²½ë¡œë¥¼ í• ë‹¹í•˜ì„¸ìš”.\n",
        "# ì˜ˆì‹œ: '/content/drive/MyDrive/Pinching_data/split_frames'\n",
        "INPUT_FRAMES_DIR = input(\"í”„ë ˆì„ ì´ë¯¸ì§€ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n",
        "\n",
        "if not os.path.isdir(INPUT_FRAMES_DIR):\n",
        "    print(f\"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì˜¬ë°”ë¥¸ ê²½ë¡œê°€ ì•„ë‹™ë‹ˆë‹¤: {INPUT_FRAMES_DIR}\")\n",
        "    exit()\n",
        "\n",
        "# INPUT_VIDEO_PATH ë³€ìˆ˜ëŠ” OUTPUT_VIDEO_PATH ìƒì„±ì„ ìœ„í•´ ì„ì‹œë¡œ í´ë”ëª…ìœ¼ë¡œ ì„¤ì •\n",
        "INPUT_VIDEO_PATH = INPUT_FRAMES_DIR\n",
        "\n",
        "print(f\"ğŸš€ ë¶„ì„í•  í”„ë ˆì„ í´ë”: {INPUT_FRAMES_DIR}\")"
      ],
      "metadata": {
        "id": "zx3gRWWG7BqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. ì˜ìƒ ë¶„ì„ ì‹¤í–‰ ë° Fusion ì˜ˆì¸¡ (í”„ë ˆì„ í´ë” ì…ë ¥)\n",
        "# ==========================================\n",
        "\n",
        "# 1. íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ë° ì •ë ¬\n",
        "if 'INPUT_FRAMES_DIR' not in locals():\n",
        "    print(\"âŒ INPUT_FRAMES_DIR ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 3-B ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
        "    exit()\n",
        "\n",
        "frame_files = sorted([f for f in os.listdir(INPUT_FRAMES_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "total_frames = len(frame_files)\n",
        "\n",
        "if total_frames == 0:\n",
        "    print(f\"âŒ í´ë” ë‚´ì—ì„œ ìœ íš¨í•œ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_FRAMES_DIR}\")\n",
        "    exit()\n",
        "\n",
        "# 2. ì˜ìƒ ë©”íƒ€ë°ì´í„° (ê°€ì •)\n",
        "# FPSì™€ WIDTH/HEIGHTëŠ” ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "first_frame_path = os.path.join(INPUT_FRAMES_DIR, frame_files[0])\n",
        "first_frame = cv2.imread(first_frame_path)\n",
        "if first_frame is None:\n",
        "    print(\"âŒ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    exit()\n",
        "\n",
        "height, width, _ = first_frame.shape\n",
        "fps = 30 # ğŸš¨ í”„ë ˆì„ ë¶„ì„ ì‹œ FPSëŠ” 30ìœ¼ë¡œ ê³ ì •í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. (ì‹¤ì œ ê°’ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥)\n",
        "\n",
        "# 3. ì¶œë ¥ ê²½ë¡œ ë° VideoWriter ì´ˆê¸°í™”\n",
        "original_filename = os.path.basename(INPUT_FRAMES_DIR).split(os.path.sep)[-1] # í´ë” ì´ë¦„ì„ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©\n",
        "OUTPUT_VIDEO_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_FRAMES_ANALYSIS.mp4\")\n",
        "PREDICTIONS_SAVE_PATH = os.path.join(SAVE_DIR, f\"{original_filename}_frames_predictions.npy\")\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n",
        "\n",
        "if not out.isOpened():\n",
        "    print(f\"âŒ VideoWriter ì´ˆê¸°í™” ì‹¤íŒ¨! ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì½”ë±ì„ ë³€ê²½í•˜ì„¸ìš”: {OUTPUT_VIDEO_PATH}\")\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "    out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n",
        "    if not out.isOpened():\n",
        "        print(\"âŒ MP4V ì½”ë±ìœ¼ë¡œë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.\")\n",
        "        exit()\n",
        "\n",
        "print(f\"ğŸ¬ í”„ë ˆì„ ë¶„ì„ ì‹œì‘ (ì´ {total_frames} í”„ë ˆì„, Skip Rate: 1 / {FRAME_SKIP_INTERVAL})\")\n",
        "print(f\"ğŸ’¾ ì¶œë ¥ ê²½ë¡œ í™•ì¸: {OUTPUT_VIDEO_PATH}\")\n",
        "pbar = tqdm(total=total_frames // FRAME_SKIP_INTERVAL)\n",
        "\n",
        "frame_counter = 0\n",
        "prev_dist = None\n",
        "stgcn_sequence = []\n",
        "all_predictions = []\n",
        "\n",
        "# 4. í”„ë ˆì„ ì´ë¯¸ì§€ ë£¨í”„ ì‹œì‘\n",
        "for filename in frame_files:\n",
        "    frame_path = os.path.join(INPUT_FRAMES_DIR, filename)\n",
        "    frame = cv2.imread(frame_path)\n",
        "    if frame is None:\n",
        "        continue # ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸°\n",
        "\n",
        "    # ì˜ìƒ ë°©í–¥ ì •ìƒí™” (ìˆ˜ì§ ë°˜ì „ - í•„ìš”í•˜ë‹¤ë©´)\n",
        "    frame = cv2.flip(frame, 0)\n",
        "\n",
        "    frame_counter += 1\n",
        "    current_frame_prediction = 0\n",
        "\n",
        "    if frame_counter % FRAME_SKIP_INTERVAL != 0:\n",
        "        out.write(frame)\n",
        "        all_predictions.append(0)\n",
        "        continue\n",
        "\n",
        "    # --- ë©”ì¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì§„ì… (ì´í•˜ ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---\n",
        "    yolo_results = yolo_model(frame, verbose=False)\n",
        "\n",
        "    is_object_present_in_frame = False\n",
        "    is_hand_detected_yolo = False\n",
        "    current_keypoints_frame = np.zeros((2, NUM_JOINTS, 4), dtype=np.float32)\n",
        "    final_kp_dnn = np.zeros(126)\n",
        "\n",
        "    cls_id_for_hand = CLS_ID_FOR_HAND\n",
        "    hand_boxes = []\n",
        "\n",
        "    for r in yolo_results:\n",
        "        for i, cls_id in enumerate(r.boxes.cls.cpu().numpy().astype(int)):\n",
        "            box = r.boxes.xyxy.cpu().numpy().astype(int)[i]\n",
        "\n",
        "            if cls_id == cls_id_for_hand:\n",
        "                hand_boxes.append(box)\n",
        "                is_hand_detected_yolo = True\n",
        "            elif cls_id != 0:\n",
        "                 is_object_present_in_frame = True\n",
        "\n",
        "    if not is_hand_detected_yolo:\n",
        "        cv2.putText(frame, \"No Hand Detected (YOLO)\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "        out.write(frame)\n",
        "        all_predictions.append(0)\n",
        "        pbar.update(1)\n",
        "        continue\n",
        "\n",
        "    is_hand_detected_mediapipe = False\n",
        "\n",
        "    for box in hand_boxes:\n",
        "        x1, y1, x2, y2 = box\n",
        "        cropped_img = frame[y1:y2, x1:x2]\n",
        "        if cropped_img.size == 0 or x2 <= x1 or y2 <= y1:\n",
        "            continue\n",
        "\n",
        "        mp_results = hands.process(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        if mp_results.multi_hand_landmarks:\n",
        "            is_hand_detected_mediapipe = True\n",
        "\n",
        "            for hand_idx, hand_landmarks in enumerate(mp_results.multi_hand_landmarks):\n",
        "                if hand_idx >= 2: continue\n",
        "\n",
        "                kp_list_dnn = []\n",
        "                for lm in hand_landmarks.landmark:\n",
        "                    kp_list_dnn.extend([lm.x, lm.y, lm.z])\n",
        "\n",
        "                if hand_idx == 0:\n",
        "                    final_kp_dnn[:len(kp_list_dnn)] = kp_list_dnn[:126]\n",
        "\n",
        "                lm_xyz = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
        "                thumb_tip = lm_xyz[4]\n",
        "                index_tip = lm_xyz[8]\n",
        "                dist = np.linalg.norm(thumb_tip - index_tip)\n",
        "\n",
        "                if dist < 0.25:\n",
        "                    modified_dist = dist * 0.2\n",
        "                else:\n",
        "                    modified_dist = dist\n",
        "\n",
        "                dist_feat = np.full((NUM_JOINTS, 1), modified_dist)\n",
        "                hand_kp_extended = np.concatenate((lm_xyz, dist_feat), axis=1)\n",
        "                current_keypoints_frame[hand_idx] = hand_kp_extended\n",
        "\n",
        "                transformed_hand_landmarks = landmark_pb2.NormalizedLandmarkList()\n",
        "                transformed_hand_landmarks.landmark.extend([\n",
        "                    landmark_pb2.NormalizedLandmark(\n",
        "                        x=(lm.x * (x2 - x1) + x1) / width,\n",
        "                        y=(lm.y * (y2 - y1) + y1) / height,\n",
        "                        z=lm.z\n",
        "                    ) for lm in hand_landmarks.landmark\n",
        "                ])\n",
        "\n",
        "                mp_drawing.draw_landmarks(frame, transformed_hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
        "                                          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "                                          mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "            if is_hand_detected_mediapipe:\n",
        "                prev_dist = dist\n",
        "\n",
        "            final_prob = 0.0\n",
        "            fusion_reason = \"DNN Only\"\n",
        "            ai_prob_dnn = dnn_model.predict(final_kp_dnn.reshape(1, -1), verbose=0)[0][0]\n",
        "\n",
        "            if stgcn_model is None:\n",
        "                final_prob = ai_prob_dnn\n",
        "                fusion_reason = \"DNN Only (STGCN Fail)\"\n",
        "            else:\n",
        "                stgcn_sequence.append(current_keypoints_frame)\n",
        "                if len(stgcn_sequence) > SEQ_LEN:\n",
        "                    stgcn_sequence.pop(0)\n",
        "\n",
        "                if len(stgcn_sequence) == SEQ_LEN:\n",
        "                    seq_arr = np.array(stgcn_sequence, dtype=np.float32)\n",
        "                    seq_arr = seq_arr.reshape(SEQ_LEN, 2, NUM_JOINTS, 4)\n",
        "                    input_data = np.transpose(seq_arr, (3, 0, 2, 1))\n",
        "                    input_data = np.expand_dims(input_data, axis=0)\n",
        "\n",
        "                    wrist_id = 0\n",
        "                    for m in range(2):\n",
        "                        wrist_xyz = input_data[0, :3, :, wrist_id, m].copy()\n",
        "                        input_data[0, :3, :, :, m] -= wrist_xyz[:, :, None]\n",
        "\n",
        "                    max_val = np.max(np.abs(input_data))\n",
        "                    if max_val > 0:\n",
        "                        input_data /= max_val\n",
        "\n",
        "                    velocity = np.zeros((1, 3, SEQ_LEN, NUM_JOINTS, 2), dtype=np.float32)\n",
        "                    velocity[:, :, 1:, :, :] = input_data[:, :3, 1:, :, :] - input_data[:, :3, :-1, :, :]\n",
        "                    final_input = np.concatenate((input_data, velocity), axis=1)\n",
        "\n",
        "                    input_tensor = torch.tensor(final_input, dtype=torch.float32).to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        outputs = stgcn_model(input_tensor)\n",
        "                        probabilities = F.softmax(outputs, dim=1)\n",
        "                        prob_stgcn = probabilities[0, 1].item()\n",
        "\n",
        "                    final_prob = (ai_prob_dnn * WEIGHT_DNN) + (prob_stgcn * WEIGHT_STGCN)\n",
        "                    fusion_reason = f\"Fusion ({WEIGHT_DNN:.1f}:{WEIGHT_STGCN:.1f})\"\n",
        "                else:\n",
        "                    final_prob = ai_prob_dnn\n",
        "                    fusion_reason = \"DNN Only (Buffering)\"\n",
        "\n",
        "            is_closing = False\n",
        "            if prev_dist is not None and (prev_dist - dist) > MIN_CLOSING_RATE:\n",
        "                is_closing = True\n",
        "\n",
        "            label = \"Non-Pinch\"\n",
        "            color = (255, 0, 0)\n",
        "            reason_text = \"Non-Pinch\"\n",
        "            current_frame_prediction = 0\n",
        "\n",
        "            if final_prob > 0.5:\n",
        "                label = f\"FINAL: PINCH ({final_prob*100:.1f}%)\"\n",
        "                color = (0, 255, 0)\n",
        "                reason_text = fusion_reason\n",
        "                current_frame_prediction = 1\n",
        "\n",
        "            elif dist < DISTANCE_THRESHOLD and is_closing and is_object_present_in_frame:\n",
        "                label = f\"FINAL: PINCH (Logic)\"\n",
        "                color = (0, 255, 255)\n",
        "                reason_text = f\"Closing + Object\"\n",
        "                current_frame_prediction = 1\n",
        "\n",
        "            display_text = f\"{label} ({reason_text})\"\n",
        "            cv2.rectangle(frame, (x1, y2 - 40), (x1 + 550, y2), color, -1)\n",
        "            cv2.putText(frame, display_text, (x1 + 10, y2 - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "    all_predictions.append(current_frame_prediction)\n",
        "    pbar.update(1)\n",
        "\n",
        "out.release()\n",
        "pbar.close()\n",
        "\n",
        "np.save(PREDICTIONS_SAVE_PATH, np.array(all_predictions))\n",
        "\n",
        "print(f\"\\nâœ¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {OUTPUT_VIDEO_PATH}\")\n",
        "print(f\"âœ… Fusion ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {PREDICTIONS_SAVE_PATH}\")"
      ],
      "metadata": {
        "id": "Wrejg_Ps7Hij"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}